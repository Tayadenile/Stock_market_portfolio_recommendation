# -*- coding: utf-8 -*-
"""final final project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_THB0SHWYY4VB7ZvtCCdgYfhVc9V-T0S
"""

!pip install pandas-ta

!pip install PyPortfolioOpt

## Step 1: Create Synthetic Dataset for Risk Profiling

import pandas as pd
import numpy as np
import os

def create_risk_profiler_dataset(num_samples=10000, filename='synthetic_risk_profiles.csv', force_overwrite=True, random_seed=42):
    """
    Creates and saves a synthetic dataset for training a user risk profiling model.
    Simulates questionnaire answers and assigns logical risk profiles.
    """
    print(f"\n--- Generating Synthetic Dataset for Risk Profiling ---")
    print(f"Creating {num_samples} synthetic user profiles...")

    np.random.seed(random_seed)  # Ensures reproducibility

    # --- Define Answer Options ---
    goals = ['Capital Protection', 'Steady Growth', 'Aggressive Wealth Creation']
    reactions = ['Sell all', 'Sell some', 'Do nothing', 'Buy more']
    experiences = ['Beginner', 'Intermediate', 'Advanced']

    # --- Generate Random Questionnaire Responses ---
    data = {
        'Age': np.random.randint(20, 71, size=num_samples),
        'Primary_Goal': np.random.choice(goals, size=num_samples, p=[0.2, 0.5, 0.3]),
        'Market_Drop_Reaction': np.random.choice(reactions, size=num_samples, p=[0.15, 0.25, 0.4, 0.2]),
        'Investment_Experience': np.random.choice(experiences, size=num_samples, p=[0.4, 0.4, 0.2])
    }

    df = pd.DataFrame(data)

    # --- Assign Risk Profiles Based on Logic ---
    df['RiskProfile'] = 'Medium'  # Default

    low_risk_mask = (df['Primary_Goal'] == 'Capital Protection') | (df['Market_Drop_Reaction'] == 'Sell all')
    high_risk_mask = (df['Primary_Goal'] == 'Aggressive Wealth Creation') | \
                     (df['Market_Drop_Reaction'] == 'Buy more') | \
                     ((df['Investment_Experience'] == 'Advanced') & (df['Market_Drop_Reaction'] != 'Sell all'))

    df.loc[low_risk_mask, 'RiskProfile'] = 'Low'
    df.loc[high_risk_mask, 'RiskProfile'] = 'High'

    # --- Save to CSV ---
    try:
        if not force_overwrite and os.path.exists(filename):
            print(f"\n‚ö†Ô∏è File '{filename}' already exists. Set `force_overwrite=True` to overwrite.")
            return

        df.to_csv(filename, index=False)
        print(f"\n‚úÖ Dataset saved to: '{filename}'")
        print("\n--- Sample Data ---")
        print(df.head())
        print("\n--- Risk Profile Distribution ---")
        print(df['RiskProfile'].value_counts())
    except Exception as e:
        print(f"‚ùå Error while saving file: {e}")

# --- Main Execution ---
if __name__ == '__main__':
    create_risk_profiler_dataset(num_samples=10000)

## Step 2: Train RandomForestClassifier for Risk Profiling

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib
import warnings

# Suppress warnings for a cleaner output
warnings.filterwarnings('ignore')

def train_risk_profiler_model(data_filename='synthetic_risk_profiles.csv'):
    """
    Trains a risk profiling model using Random Forest.
    Saves the trained model and the one-hot encoded column structure.
    """
    print("--- Starting Model Training and Evaluation ---")

    # 1. Load Dataset
    try:
        df = pd.read_csv(data_filename)
        print("‚úÖ Dataset loaded successfully.")
    except FileNotFoundError:
        print(f"‚ùå Error: File '{data_filename}' not found.")
        return
    except Exception as e:
        print(f"‚ùå Error reading file: {e}")
        return

    # 2. Preprocessing (One-Hot Encoding)
    print("\nüîß Preprocessing data (One-Hot Encoding)...")
    X = df.drop('RiskProfile', axis=1)
    y = df['RiskProfile']
    X_encoded = pd.get_dummies(X, columns=['Primary_Goal', 'Market_Drop_Reaction', 'Investment_Experience'], drop_first=True)

    # Save model input column names
    joblib.dump(X_encoded.columns.tolist(), 'risk_model_columns.joblib')
    print("üì¶ Feature columns saved to 'risk_model_columns.joblib'")

    # 3. Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, stratify=y, random_state=42)

    # 4. Model Training
    print("\nüöÄ Training Random Forest Classifier...")
    model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
    model.fit(X_train, y_train)
    print("‚úÖ Model training complete.")

    # 5. Evaluation
    print("\nüìä Evaluating Model...")
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"‚úÖ Accuracy: {acc:.4f}")

    print("\nüß© Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    print("\nüìÑ Classification Report:")
    print(classification_report(y_test, y_pred))

    # 6. Save Model
    model_filename = 'risk_profiler_model.joblib'
    joblib.dump(model, model_filename)
    print(f"\nüíæ Model saved to '{model_filename}'")

    print("\n‚úÖ Done! You now have a trained risk profiling model ready to use.")

# Run when script is executed
if __name__ == '__main__':
    train_risk_profiler_model()

## Step 3: Create Synthetic dataset for Asset Allocation

import pandas as pd
import numpy as np
import os
from datetime import datetime

def create_allocator_dataset(num_samples=10000, filename='synthetic_investor_profiles.csv'):
    """
    Creates a synthetic dataset for training an asset allocation model using K-Means clustering.
    The dataset includes Age, Investment Horizon, and Risk Profile (as input from Risk Model).
    """
    print("\nüöÄ --- Generating Synthetic Dataset for Asset Allocator (Model 2) ---")

    # 1. Define Possible Risk Profiles (output from Model 1)
    risk_profiles = ['Low', 'Medium', 'High']
    print(f"üìå Using risk profile distribution: Low (30%), Medium (40%), High (30%)")

    # 2. Generate Synthetic Data
    print(f"üîÑ Creating {num_samples:,} investor profiles with random attributes...")
    data = {
        'Age': np.random.randint(20, 71, size=num_samples),
        'Investment_Horizon_Yrs': np.random.randint(1, 41, size=num_samples),
        'RiskProfile': np.random.choice(risk_profiles, size=num_samples, p=[0.3, 0.4, 0.3])
    }
    df = pd.DataFrame(data)

    # 3. Save to CSV File
    try:
        df.to_csv(filename, index=False)
        print(f"\nüíæ Successfully saved dataset to: {os.path.abspath(filename)}")
        print("üß™ Preview of generated dataset:\n")
        print(df.head())
    except Exception as e:
        print(f"‚ùå Error saving file: {e}")

    # 4. Optional: Log file creation time
    print(f"\nüïí Dataset generation completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# --- Main Execution ---
if __name__ == '__main__':
    create_allocator_dataset(num_samples=10000)

## Step 4: Train K-Means Clustering Model for Asset Allocation

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
import joblib
import warnings

# Suppress warnings for a cleaner output
warnings.filterwarnings('ignore')

def train_allocator_model(data_filename='synthetic_investor_profiles.csv'):
    """
    Loads investor data, trains a K-Means model to create investor personas,
    and maps these personas to asset allocation strategies.
    """
    print("--- Starting Asset Allocator Model Training ---")

    # 1. Load the Dataset
    try:
        df = pd.read_csv(data_filename)
        print("Dataset loaded successfully.")
    except FileNotFoundError:
        print(f"Error: The file '{data_filename}' was not found. Please run the data generation script first.")
        return
    except Exception as e:
        print(f"An error occurred while reading the file: {e}")
        return

    # 2. Preprocess the Data for Clustering
    print("\nPreprocessing data for clustering...")

    # Create a copy to keep the original data for analysis later
    df_processed = df.copy()

    # Convert 'RiskProfile' text to numbers (e.g., Low=0, Medium=1, High=2)
    le = LabelEncoder()
    df_processed['RiskProfile_Encoded'] = le.fit_transform(df_processed['RiskProfile'])

    # Select features for clustering
    features = df_processed[['Age', 'Investment_Horizon_Yrs', 'RiskProfile_Encoded']]

    # Scale the features. This is critical for K-Means to work correctly,
    # as it prevents features with larger scales (like Age) from dominating.
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    # Save the scaler so we can apply the exact same transformation to new user data
    joblib.dump(scaler, 'allocator_scaler.joblib')
    print("Scaler saved to 'allocator_scaler.joblib'")

    # 3. Find the Optimal Number of Clusters (K) using the Elbow Method
    print("\nFinding optimal K using the Elbow Method...")
    wcss = [] # Within-Cluster Sum of Squares
    for i in range(1, 11):
        kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)
        kmeans.fit(features_scaled)
        wcss.append(kmeans.inertia_)

    # Plotting the Elbow Method graph
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
    plt.title('Elbow Method for Optimal K')
    plt.xlabel('Number of Clusters (K)')
    plt.ylabel('WCSS (Inertia)')
    plt.grid(True)
    plt.show()

    # We are setting K=5 to encourage a dedicated low-risk cluster.
    optimal_k = 5
    print(f"Based on the plot and our goal, we will proceed with K = {optimal_k}")

    # 4. Train the Final K-Means Model
    print(f"\nTraining final K-Means model with K={optimal_k}...")
    kmeans_model = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42, n_init=10)
    kmeans_model.fit(features_scaled)

    # Save the trained clustering model
    joblib.dump(kmeans_model, 'allocator_model.joblib')
    print("Trained K-Means model saved to 'allocator_model.joblib'")

    # 5. Analyze Clusters and Define Asset Allocation
    print("\nAnalyzing clusters to create investor personas...")
    df['Cluster'] = kmeans_model.labels_

    # Calculate the average characteristics for each cluster
    cluster_analysis = df.groupby('Cluster')[['Age', 'Investment_Horizon_Yrs']].mean().round(2)
    cluster_analysis['Dominant_RiskProfile'] = df.groupby('Cluster')['RiskProfile'].agg(lambda x: x.mode()[0])

    print("\n--- Cluster Analysis Results ---")
    print(cluster_analysis)

    # IMPORTANT: Based on the analysis above, you manually define personas and map allocations.
    # This step requires human interpretation of the model's output.
    print("\nDefining and mapping asset allocation strategies for 5 clusters...")

    # We define 5 allocation strategies. You will need to look at your new
    # "Cluster Analysis Results" and manually assign the correct cluster number
    # (0, 1, 2, 3, 4) to the most logical allocation strategy below after you run this script.
    allocation_map = {
        # Cluster Number: {'Equity': %, 'Debt': %, 'Gold': %}
        0: {'Equity': 80, 'Debt': 15, 'Gold': 5},   # Persona: Aggressive Growth
        1: {'Equity': 65, 'Debt': 30, 'Gold': 5},   # Persona: Balanced Growth
        2: {'Equity': 45, 'Debt': 50, 'Gold': 5},   # Persona: Cautious / Balanced
        3: {'Equity': 25, 'Debt': 65, 'Gold': 10},  # Persona: Conservative / Low Risk
        4: {'Equity': 50, 'Debt': 45, 'Gold': 5}    # Persona: Another variation of Balanced
    }

    joblib.dump(allocation_map, 'allocation_map.joblib')
    print("Asset allocation map for 5 clusters saved to 'allocation_map.joblib'")

    print("\n--- Process Complete ---")
    print("You now have a trained clustering model for 5 personas.")

# --- Main Execution Block ---
if __name__ == '__main__':
    train_allocator_model()

## Step 5: Asset Allocation Model Testing

import pandas as pd
import joblib

def predict_asset_allocation(user_profile):
    """
    Predict asset allocation for a new user using a pre-trained KMeans model.

    Args:
        user_profile (dict): {'Age': int, 'Investment_Horizon_Yrs': int, 'RiskProfile': str}

    Returns:
        dict: Recommended asset allocation percentages.
    """
    print("üöÄ Loading model files...")
    try:
        kmeans_model = joblib.load('allocator_model.joblib')
        scaler = joblib.load('allocator_scaler.joblib')
        allocation_map = joblib.load('allocation_map.joblib')
    except FileNotFoundError as e:
        print(f"‚ùå File not found: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Error loading model components: {e}")
        return None

    print("\nüìå Input Investor Profile:")
    print(user_profile)

    # Step 1: DataFrame Conversion
    df = pd.DataFrame([user_profile])

    # Step 2: Encode 'RiskProfile'
    risk_map = {'Low': 0, 'Medium': 1, 'High': 2}
    if df['RiskProfile'][0] not in risk_map:
        print("‚ùå Invalid RiskProfile value. Must be 'Low', 'Medium', or 'High'.")
        return None

    df['RiskProfile_Encoded'] = df['RiskProfile'].map(risk_map)

    # Step 3: Feature Selection
    features = df[['Age', 'Investment_Horizon_Yrs', 'RiskProfile_Encoded']]

    # Step 4: Scaling
    scaled_features = scaler.transform(features)

    # Step 5: Prediction
    predicted_cluster = kmeans_model.predict(scaled_features)[0]
    print(f"\nüîç Predicted Investor Persona Cluster: {predicted_cluster}")

    # Step 6: Allocation Lookup
    allocation = allocation_map.get(predicted_cluster)
    if allocation:
        print(f"\n‚úÖ Recommended Asset Allocation: {allocation}")
    else:
        print("‚ùå No allocation strategy found for this cluster.")

    return allocation

# ------------------------------
# üî¨ Example Inference Scenarios
# ------------------------------
if __name__ == '__main__':
    print("\n================ User 1 ================\n")
    user_1 = {
        'Age': 22,
        'Investment_Horizon_Yrs': 20,
        'RiskProfile': 'Low'
    }
    predict_asset_allocation(user_1)

    print("\n================ User 2 ================\n")
    user_2 = {
        'Age': 58,
        'Investment_Horizon_Yrs': 7,
        'RiskProfile': 'High'
    }
    predict_asset_allocation(user_2)

## Step 6: Combine 'Close' price of all stocks into one single dataframe

import pandas as pd
import os
import json

def load_and_prepare_stock_data(folder_path='/content/drive/MyDrive/stock_data_json'):
    """
    Loads historical stock data from JSON files in the given folder, extracts
    the closing prices, and combines them into a single time-series DataFrame.

    Args:
        folder_path (str): Directory containing JSON files for each stock.

    Returns:
        pd.DataFrame: Combined DataFrame with Date as index and tickers as columns (closing prices).
    """
    print(f"üìÇ Loading stock data from: '{folder_path}'")

    if not os.path.isdir(folder_path):
        print(f"‚ùå Error: Folder '{folder_path}' not found.")
        print("üìå Please ensure 99 JSON files are placed correctly.")
        return None

    json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]

    if not json_files:
        print(f"‚ö†Ô∏è No JSON files found in '{folder_path}'.")
        return None

    print(f"‚úÖ Found {len(json_files)} JSON files.\n")

    combined_data = []

    for i, filename in enumerate(json_files, start=1):
        ticker = filename.replace('.json', '')
        file_path = os.path.join(folder_path, filename)

        try:
            df = pd.read_json(file_path)
            df['Date'] = pd.to_datetime(df['Date']).dt.tz_localize(None)
            df.set_index('Date', inplace=True)
            df = df[['Close']].rename(columns={'Close': ticker})
            combined_data.append(df)

            print(f"üìà [{i}/{len(json_files)}] Processed: {ticker}")

        except Exception as e:
            print(f"‚ö†Ô∏è Skipping '{filename}' due to error: {e}")

    if not combined_data:
        print("‚ùå No stock data processed successfully.")
        return None

    print("\nüîó Merging individual stock dataframes...")
    master_df = pd.concat(combined_data, axis=1)
    master_df.sort_index(inplace=True)

    output_file = 'all_stocks_close_prices.csv'
    master_df.to_csv(output_file)

    print(f"\n‚úÖ Combined closing price data for {len(master_df.columns)} stocks.")
    print(f"üíæ Saved to: '{output_file}'")

    print("\nüîç Data Preview:")
    print(master_df.tail())

    return master_df


# --- Main Block ---
if __name__ == '__main__':
    df_combined = load_and_prepare_stock_data()

## Step 7: Combine 'Close' price of all Mutual Funds into one single dataframe

import pandas as pd
import os
import json
from tqdm import tqdm

def load_mutual_fund_navs(folder_path='/content/sample_data/mf'):
    """
    Loads NAV history from mutual fund JSON files and combines into one DataFrame.

    Args:
        folder_path (str): Path where all mutual fund JSONs are stored.

    Returns:
        pd.DataFrame: Combined DataFrame with NAVs; index = date, columns = fund names.
    """

    print(f"\nüìÇ Reading from folder: {folder_path}\n")

    if not os.path.isdir(folder_path):
        print("‚ùå Folder not found.")
        return None

    all_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]
    if not all_files:
        print("‚ö†Ô∏è No JSON files found.")
        return None

    nav_frames = []

    for file in tqdm(all_files, desc="‚è≥ Processing JSONs"):
        try:
            filepath = os.path.join(folder_path, file)
            with open(filepath, 'r') as f:
                data = json.load(f)

            # Extract fund name from metadata
            fund_name = data['metadata']['name'] if 'name' in data['metadata'] else file.replace('.json', '')

            # Extract NAV history
            nav_history = data.get('nav_history', [])
            df = pd.DataFrame(nav_history)

            # Standardize
            df['date'] = pd.to_datetime(df['date'], format='%d-%b-%Y')
            df.set_index('date', inplace=True)
            df = df[['nav']].rename(columns={'nav': fund_name})

            nav_frames.append(df)

        except Exception as e:
            print(f"‚ö†Ô∏è Error in file '{file}': {e}")

    if not nav_frames:
        print("‚ùå No valid NAV data processed.")
        return None

    print(f"\nüîó Merging {len(nav_frames)} mutual funds...")
    combined_df = pd.concat(nav_frames, axis=1)
    combined_df.sort_index(inplace=True)

    # Optional: Save to CSV
    combined_df.to_csv('combined_mutual_fund_navs.csv')
    print("‚úÖ Combined NAV data saved as 'combined_mutual_fund_navs.csv'")

    print("\nüß™ Preview:")
    print(combined_df.tail())

    return combined_df


# Run
if __name__ == '__main__':
    df_nav = load_mutual_fund_navs()

## Step 8: Prophet for Stocks

import pandas as pd
from prophet import Prophet
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error
import numpy as np
np.NaN = np.nan
import os
import json
import pandas_ta as ta
import warnings

warnings.filterwarnings('ignore', category=FutureWarning)
np.NaN = np.nan

def smape(y_true, y_pred):
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    diff = np.abs(y_true - y_pred) / denominator
    diff[denominator == 0] = 0.0
    return 100 * np.mean(diff)

def evaluate_stock_metrics(json_path, test_days=365, momentum_years=5, forecast_days=365):
    ticker = os.path.basename(json_path).replace('.json', '')
    print(f"-> Processing: {ticker}")

    try:
        with open(json_path, 'r') as f: data = json.load(f)
    except Exception as e:
        print(f"  ERROR: Could not read file. {e}")
        return None

    historical_data = None
    if isinstance(data, list): historical_data = data
    elif isinstance(data, dict):
        if ticker in data: historical_data = data[ticker]
        elif data: historical_data = data[next(iter(data))]
    if historical_data is None:
        print(f"  ERROR: Could not parse data for {ticker}.")
        return None

    df_full = pd.DataFrame(historical_data)
    df_full = df_full.rename(columns={'Date': 'ds', 'Close': 'y', 'Volume': 'volume'})
    df_full['ds'] = pd.to_datetime(df_full['ds'], format='mixed', dayfirst=True).dt.tz_localize(None)
    df_full['y'] = pd.to_numeric(df_full['y'], errors='coerce')
    if 'volume' not in df_full.columns:
        df_full['volume'] = 0
    df_full = df_full.dropna(subset=['ds', 'y']).drop_duplicates(subset=['ds']).sort_values(by='ds')

    if len(df_full) < test_days + 52:
        print("  WARNING: Not enough historical data. Skipping.")
        return None

    df_full['y'] = np.log(df_full['y'])

    df_full['SMA_50'] = ta.sma(df_full['y'], length=50)
    df_full['RSI_14'] = ta.rsi(df_full['y'], length=14)
    bbands = ta.bbands(df_full['y'], length=20, std=2)
    if bbands is not None and not bbands.empty:
        df_full = df_full.join(bbands.rename(columns={'BBL_20_2.0': 'BBL', 'BBM_20_2.0': 'BBM', 'BBU_20_2.0': 'BBU'}))
    df_full = df_full.dropna().reset_index(drop=True)

    # PART A: Full historical evaluation
    train_eval_df = df_full.iloc[:-test_days]
    test_eval_df = df_full.iloc[-test_days:]
    regressors = ['volume', 'SMA_50', 'RSI_14', 'BBL', 'BBM', 'BBU']
    active_regressors = [reg for reg in regressors if reg in df_full.columns and df_full[reg].nunique() > 1]

    model_eval = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=True,
        daily_seasonality=False,
        seasonality_mode='multiplicative',
        changepoint_prior_scale=0.01
    )
    model_eval.add_country_holidays(country_name='India')
    for reg in active_regressors:
        model_eval.add_regressor(reg)
    model_eval.fit(train_eval_df)

    future_eval_df = test_eval_df[['ds'] + active_regressors]
    forecast_eval = model_eval.predict(future_eval_df)

    y_true = np.exp(test_eval_df['y'].values)
    y_pred = np.exp(forecast_eval['yhat'].values)
    r2 = r2_score(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = mean_absolute_percentage_error(y_true, y_pred) * 100
    smape_val = smape(y_true, y_pred)

    if r2 < 0.75:
        print(f"  Skipped: R¬≤ too low ({r2:.2f})")
        return None

    # PART B: Forecasting using last 5 years of momentum
    momentum_cutoff_date = df_full['ds'].max() - pd.DateOffset(years=momentum_years)
    df_momentum = df_full[df_full['ds'] >= momentum_cutoff_date].reset_index(drop=True)

    if len(df_momentum) < 52:
        print("  WARNING: Not enough data in the momentum window. Skipping forecast.")
        return None

    model_forecast = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=True,
        daily_seasonality=False,
        seasonality_mode='multiplicative',
        changepoint_prior_scale=0.01
    )
    model_forecast.add_country_holidays(country_name='India')
    for reg in active_regressors:
        model_forecast.add_regressor(reg)
    model_forecast.fit(df_momentum)

    future_forecast_df = model_forecast.make_future_dataframe(periods=forecast_days)

    np.random.seed(42)
    last_regressor_values = df_momentum[active_regressors].iloc[-1]
    for reg in active_regressors:
        val = last_regressor_values[reg]
        noise = np.random.normal(loc=0, scale=0.01 * abs(val), size=len(future_forecast_df))
        future_forecast_df[reg] = val + noise

    forecast_future = model_forecast.predict(future_forecast_df).reset_index(drop=True)

    last_price = np.exp(df_momentum['y'].iloc[-1])
    predicted_price = np.exp(forecast_future['yhat'].iloc[-1])
    forecasted_cagr = ((predicted_price / last_price) ** (1/1) - 1) * 100

    hist_cutoff = df_full['ds'].max() - pd.DateOffset(years=momentum_years)
    df_hist = df_full[df_full['ds'] >= hist_cutoff].reset_index(drop=True)
    if len(df_hist) < 2:
        return None
    hist_start = np.exp(df_hist['y'].iloc[0])
    hist_end = np.exp(df_hist['y'].iloc[-1])
    hist_cagr = ((hist_end / hist_start) ** (1/momentum_years) - 1) * 100

    return {
        'Ticker': ticker,
        'R2_Score': round(r2, 4),
        'RMSE': round(rmse, 2),
        'MAPE': round(mape, 2),
        'SMAPE': round(smape_val, 2),
        'Forecasted_1Y_CAGR': round(forecasted_cagr, 2),
        f'Historical_{momentum_years}Y_CAGR': round(hist_cagr, 2)
    }

# üöÄ Main block for batch processing
if __name__ == "__main__":
    folder_path = '/content/sample_data/stocks'  # üîÅ Change as needed
    output_csv = 'stock_forecast_results.csv'

    results = []
    for file in os.listdir(folder_path):
        if file.endswith('.json'):
            full_path = os.path.join(folder_path, file)
            result = evaluate_stock_metrics(full_path, momentum_years=5)
            if result:
                results.append(result)

    if results:
        df_results = pd.DataFrame(results)
        df_results.to_csv(output_csv, index=False)
        print(f"\n‚úÖ Saved {len(df_results)} results to '{output_csv}'")
    else:
        print("\n‚ö†Ô∏è No valid results to save.")

## Step 9: Prophet for Mutual Funds

import pandas as pd
from prophet import Prophet
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import os
import json
import pandas_ta as ta # Import the technical analysis library

# List to store the performance results for each fund
results = []

def smape(y_true, y_pred):
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    diff = np.abs(y_true - y_pred) / denominator
    diff[denominator == 0] = 0.0
    return 100 * np.mean(diff)

def forecast_single_fund(json_filename, test_days=365):
    """
    Loads historical mutual fund NAV data from a JSON file, adds technical indicators,
    evaluates a Prophet model, and returns metrics only if R-squared is >= 0.75.
    """

    # 1. Load and parse the JSON file
    try:
        with open(json_filename, 'r') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Error reading {os.path.basename(json_filename)}: {e}")
        return None

    # 2. Extract data and fund name
    fund_name = None
    historical_data = None

    if "metadata" in data and isinstance(data['metadata'], list) and "nav_history" in data and isinstance(data['nav_history'], dict):
        fund_name = data['metadata'][0].get('name', os.path.basename(json_filename))
        historical_data = data["nav_history"].get("historical_nav")
        fund_df = pd.DataFrame(historical_data, columns=['ds', 'y'])

    elif "metadata" in data and isinstance(data['metadata'], dict) and "nav_history" in data and isinstance(data['nav_history'], list):
        fund_name = data['metadata'].get('name', os.path.basename(json_filename))
        historical_data = data["nav_history"]
        fund_df = pd.DataFrame(historical_data)
        fund_df = fund_df.rename(columns={'date': 'ds', 'nav': 'y'})

    else:
        print(f"Error: Could not parse the structure of {os.path.basename(json_filename)}. Skipping.")
        return None

    print(f"--- Starting Forecast for: {fund_name} ---")

    # 3. Prepare and validate the data
    if fund_df.empty:
        return None

    fund_df['ds'] = pd.to_datetime(fund_df['ds'], format='mixed', dayfirst=True).dt.tz_localize(None)
    fund_df['y'] = pd.to_numeric(fund_df['y'], errors='coerce')
    fund_df = fund_df.dropna()

    # Calculate Technical Indicators
    fund_df['SMA_50'] = ta.sma(fund_df['y'], length=50)
    fund_df['RSI_14'] = ta.rsi(fund_df['y'], length=14)
    bbands = ta.bbands(fund_df['y'], length=20, std=2)
    if bbands is not None and not bbands.empty:
        bbands = bbands.rename(columns={'BBL_20_2.0': 'BBL', 'BBM_20_2.0': 'BBM', 'BBU_20_2.0': 'BBU'})
        fund_df = fund_df.join(bbands)

    fund_df = fund_df.dropna().reset_index(drop=True)

    if len(fund_df) < test_days + 52:
        return None

    print(f"Data and technical indicators prepared. Total records: {len(fund_df)}")

    # 4. Split data for model evaluation
    train_df = fund_df.iloc[:-test_days]
    test_df = fund_df.iloc[-test_days:]
    print(f"Splitting data: {len(train_df)} training points, {len(test_df)} testing points.")

    # 5. Initialize and train the Prophet model
    eval_model = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)
    eval_model.add_country_holidays(country_name='India')

    regressors = ['SMA_50', 'RSI_14', 'BBL', 'BBM', 'BBU']
    for reg in regressors:
        if reg in fund_df.columns:
            eval_model.add_regressor(reg)

    eval_model.fit(train_df)

    # 6. Make predictions and calculate performance
    future_df = test_df[['ds'] + [reg for reg in regressors if reg in test_df.columns]]
    test_forecast = eval_model.predict(future_df)

    y_true = test_df['y'].values
    y_pred = test_forecast['yhat'].values

    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    smape_val = smape(y_true, y_pred)

    print("\n--- Performance Metrics ---")
    print(f"üìà RMSE: {rmse:.2f}")
    print(f"üìä MAPE: {mape:.2f}%")
    print(f"üîÅ SMAPE: {smape_val:.2f}%")
    print(f"üéØ R-squared: {r2:.2f}")

    if r2 >= 0.75:
        # Forecast next 1 year using full model
        full_model = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)
        full_model.add_country_holidays(country_name='India')
        for reg in regressors:
            if reg in fund_df.columns:
                full_model.add_regressor(reg)
        full_model.fit(fund_df)

        future_full = fund_df[['ds'] + [reg for reg in regressors if reg in fund_df.columns]].copy()
        last_date = fund_df['ds'].max()
        future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=365)
        last_row = fund_df.iloc[-1]
        new_future = pd.DataFrame({'ds': future_dates})
        for reg in regressors:
            if reg in fund_df.columns:
                new_future[reg] = last_row[reg]
        future_full = pd.concat([future_full, new_future], ignore_index=True)
        forecast_full = full_model.predict(future_full)

        nav_today = fund_df['y'].iloc[-1]
        nav_next_year = forecast_full.loc[forecast_full['ds'] == forecast_full['ds'].max(), 'yhat'].values[0]
        forecasted_cagr = ((nav_next_year / nav_today) ** (1 / 1) - 1) * 100

        # Historical 5Y CAGR
        past_date = fund_df['ds'].max() - pd.DateOffset(years=5)
        past_data = fund_df[fund_df['ds'] <= past_date]
        if not past_data.empty:
            nav_past = past_data.iloc[-1]['y']
            nav_now = fund_df['y'].iloc[-1]
            historical_cagr = ((nav_now / nav_past) ** (1 / 5) - 1) * 100
        else:
            historical_cagr = np.nan

        print(f"üìå Forecasted 1Y CAGR: {forecasted_cagr:.2f}%")
        print(f"üìå Historical 5Y CAGR: {historical_cagr:.2f}%")
        print("‚úÖ PASSED: R2 score is above the 75% threshold. Adding to results.")
        print("--------------------------\n")

        return {
            'Fund Name': fund_name,
            'RMSE': rmse,
            'MAPE': mape,
            'SMAPE': smape_val,
            'R2_Score': r2,
            'Forecasted_1Y_CAGR (%)': forecasted_cagr,
            'Historical_5Y_CAGR (%)': historical_cagr
        }
    else:
        print("‚ùå FAILED: R2 score is below the 75% threshold. Discarding this forecast.")
        print("--------------------------\n")
        return None

# --- Main Processing Loop ---
folder_path = '/content/sample_data/mf'

if not os.path.isdir(folder_path):
    print(f"Error: The folder '{folder_path}' does not exist.")
else:
    for filename in os.listdir(folder_path):
        if filename.endswith('.json'):
            file_path = os.path.join(folder_path, filename)
            result = forecast_single_fund(file_path)
            if result:
                results.append(result)

    if results:
        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values(by='R2_Score', ascending=False)
        results_df.to_csv('mutual_fund_forecast_metrics_filtered.csv', index=False)
        print("\n--- ‚úÖ All Forecasts Complete ---")
        print(f"Saved {len(results_df)} high-quality forecasts to 'mutual_fund_forecast_metrics_filtered.csv'")
    else:
        print("No mutual fund forecasts met the R2 >= 0.75 quality threshold.")

## Step 10: Give Risk Scores to Stocks

import pandas as pd
import yfinance as yf
import numpy as np
from datetime import datetime, timedelta
from tqdm import tqdm

# Load CSV
df = pd.read_excel("/content/sample_data/New_Stocklist.xlsx")  # Replace with your file name
tickers = df['yfinance Ticker'].dropna().unique()

# Date range for 1-year volatility
end_date = datetime.today()
start_date = end_date - timedelta(days=365)

results = []

for ticker in tqdm(tickers):
    try:
        stock = yf.Ticker(ticker)
        info = stock.info

        beta = info.get('beta', None)

        # Get historical data
        hist = stock.history(start=start_date.strftime('%Y-%m-%d'), end=end_date.strftime('%Y-%m-%d'))

        volatility = None
        if not hist.empty:
            hist['Return'] = hist['Close'].pct_change()
            volatility = hist['Return'].std() * np.sqrt(252)

        # Risk classification
        if beta is None and volatility is None:
            risk = 'Unknown'
        else:
            # Use whichever is available
            if beta is not None and volatility is not None:
                if beta < 0.9 and volatility < 0.25:
                    risk = 'Low Risk'
                elif beta > 1.2 or volatility > 0.4:
                    risk = 'High Risk'
                else:
                    risk = 'Medium Risk'
            elif beta is not None:
                if beta < 0.9:
                    risk = 'Low Risk'
                elif beta > 1.2:
                    risk = 'High Risk'
                else:
                    risk = 'Medium Risk'
            elif volatility is not None:
                if volatility < 0.25:
                    risk = 'Low Risk'
                elif volatility > 0.4:
                    risk = 'High Risk'
                else:
                    risk = 'Medium Risk'

        results.append({
            'Ticker': ticker,
            'Beta': round(beta, 2) if beta is not None else None,
            'Volatility': round(volatility, 4) if volatility is not None else None,
            'Risk Category': risk
        })

    except Exception as e:
        print(f"‚ö†Ô∏è Error processing {ticker}: {e}")

# Save results
risk_df = pd.DataFrame(results)
risk_df.to_csv("Stock_Risk_Categories.csv", index=False)
print("‚úÖ Done. File saved as 'Stock_Risk_Categories.csv'")

## Step 11: Categorize Mutual Funds

import os
import json
import pandas as pd

# üîÅ Update this to your folder path
json_folder = "/content/sample_data/mf"

# List to store all results
mf_data_list = []

# Loop through all JSON files
for filename in os.listdir(json_folder):
    if filename.endswith(".json"):
        file_path = os.path.join(json_folder, filename)
        try:
            with open(file_path, "r") as f:
                data = json.load(f)

                meta = data.get("metadata", {})
                mf_data_list.append({
                    "ISIN": meta.get("ISIN"),
                    "Scheme_Name": meta.get("name"),
                    "Fund_Category": meta.get("fund_category"),
                    "Fund_Type": meta.get("fund_type"),
                    "Riskometer": meta.get("crisil_rating"),
                })
        except Exception as e:
            print(f"‚ö†Ô∏è Error processing {filename}: {e}")

# Convert to DataFrame
df_meta = pd.DataFrame(mf_data_list)

# Save to CSV
df_meta.to_csv("Mutual_Fund_Metadata.csv", index=False)
print("‚úÖ Saved: 'Mutual_Fund_Metadata.csv'")

